{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sklearn scikit-image h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "import json\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "# map\n",
    "with open(\"data02/database/database_lite.json\",\"r\") as f:\n",
    "    m_idx = json.load(f)\n",
    "    m_imgs = np.array(m_idx[\"im_paths\"])\n",
    "    m_loc=np.array(m_idx[\"loc\"])\n",
    "\n",
    "# query\n",
    "with open(\"data02/query/query_lite.json\",\"r\") as f:\n",
    "    q_idx=json.load(f)\n",
    "    q_imgs=np.array(q_idx[\"im_paths\"])\n",
    "    q_loc=np.array(q_idx[\"loc\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create descriptors\n",
    "import os\n",
    "import skimage\n",
    "from skimage.feature import ORB\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "# Initialize the ORB descriptor\n",
    "descriptor_extractor = ORB(n_keypoints=10000)\n",
    "# Initialize the data structure that will contain all the descriptors\n",
    "descriptors = []\n",
    "responses = []\n",
    "\n",
    "# Loop over map images\n",
    "for img_name in m_imgs:\n",
    "    img = plt.imread(os.path.join('data02/', img_name))\n",
    "    img = rgb2gray(img)\n",
    "    \n",
    "    # Extract ORB descriptors\n",
    "    descriptor_extractor.detect_and_extract(img)  \n",
    "    # keypoints1 = descriptor_extractor.keypoints  # position of the points (not interesting for us)\n",
    "    descriptors_img = descriptor_extractor.descriptors  # descriptors (the feature vectors)\n",
    "    responses_img = descriptor_extractor.responses\n",
    "    # Accumulate the computed descriptors\n",
    "    descriptors.append(descriptors_img)\n",
    "    responses.append(responses_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ns for which a subset is created\n",
    "ns = [50, 100, 250, 500, 750, 1000, 1250, 1500, 1750, 2000]\n",
    "descriptors_ns = [] # will contain subsets of the descriptors for each N in Ns\n",
    "for n in ns:\n",
    "    descriptors_n = []\n",
    "    for index in range(len(descriptors)):\n",
    "        best_indices = responses[index].argsort()[::-1][:n]\n",
    "        descriptors_n.extend(descriptors[index][best_indices])\n",
    "    descriptors_ns.append(descriptors_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# save descriptors (uncomment if you want to save the computed descriptors)\n",
    "#f = open('./ORB-descriptors-original.bin', 'wb')\n",
    "#data = pickle.dump(descriptors, f)\n",
    "#f.close()\n",
    "#f = open('./ORB-descriptors-for-ns.bin', 'wb')\n",
    "#data = pickle.dump(descriptors_ns, f)\n",
    "#f.close()\n",
    "\n",
    "# load pre-computed descriptors\n",
    "f = open('./ORB-descriptors-original.bin', 'rb')\n",
    "descriptors = pickle.load(f)\n",
    "f.close()\n",
    "f = open('./ORB-descriptors-for-ns.bin', 'rb')\n",
    "descriptors_ns = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# clustering\n",
    "K = 32  # number of clusters (equivalent to the number of words) we want to estimate\n",
    "num_initialization = 5 # Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "\n",
    "# Run the k-means clustering\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=num_initialization, verbose=1)\n",
    "clusters = kmeans.fit(descriptors_ns[9])  # we use the descriptors extracted from the map (training) images before\n",
    "centroids = clusters.cluster_centers_\n",
    "\n",
    "print(\"Shape of the centroids matrix: \", centroids.shape)\n",
    "print(\"We computed \", centroids.shape[0], \"centroids of lengh \", centroids.shape[1], \" (the same of the descriptor)\")\n",
    "# Rememeber: the centroids can be considered as the words that compose our documents \n",
    "# -> in this case the basic components of the images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fc1f99f643323e401d5e4676f73e083ad12419fabd7b003a3f53ab94d63fb0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
